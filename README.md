# Voice-Driven MR Accessibility MVP

A minimal Unity prototype enabling **voice-based interactions** for individuals with low muscle tone who struggle to use traditional VR controllers. Created as part of a Master’s thesis focusing on **inclusive, user-centered design** in Mixed Reality.

## Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Installation](#installation)
- [Usage](#usage)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

---

## Overview
This project is a **proof-of-concept** Unity scene showcasing voice-driven navigation and commands for a brief **therapy** or **creative** scenario. The MVP aims to:
- Reduce **physical strain** for users with low muscle tone.
- Demonstrate **empathy** in error handling and feedback.
- Provide minimal but **functional** voice commands for basic tasks (e.g., “Start session,” “Next step,” “End session”).

### Why This Matters
Users with motor-control challenges often find VR controllers burdensome or impossible. By leveraging voice (via Wit.ai or a similar service), this MVP opens XR interactions to a broader audience, emphasizing **accessibility, user autonomy,** and **privacy**.

---

## Key Features
1. **Basic Voice Commands**  
   - Three to five essential commands controlling session flow.
2. **Empathetic Error Handling**  
   - The system apologizes, re-prompts, or offers synonyms when voice recognition fails.
3. **Minimal Environment**  
   - A simplified 3D scene with minimal art assets to keep focus on function, not form.
4. **Optional Data Settings** (Planned)  
   - Potential toggles for local vs. cloud-based processing (depending on feasibility).

---

## Installation
1. **Clone the Repository**  
   ```bash
   git clone https://github.com/mikechaves/a11y-voice-xr.git
